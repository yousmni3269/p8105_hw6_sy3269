---
title: "P8105_HW6_sy3269"
author: "Soomin You"
output: github_document
---

Packages required for this homework, such as `tidyverse` and `readr`, are loaded. 

```{r setup, include = FALSE, message = FALSE}
library(tidyverse)
library(readr)
library(modelr)
library(mgcv)
library(glmnet)

set.seed(10)
```


## Problem 1

The weather data required for this problem 1 is imported. 

```{r weather_data_import}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

Using the `tmax` as the response and `tmin` as the predictor, bootstrapping is 
performed and estimates for ð‘ŸÌ‚2 and log(ð›½Ì‚0âˆ—ð›½Ì‚1) for each bootstrap sample are
determined. 

```{r}
boot_straps = 
  weather_df |>
  modelr::bootstrap(5000) |>
  mutate(
    strap = map(strap, as_tibble), 
    models = map(strap, \(df) lm(tmax ~ tmin, data = df)), 
    results_tidy = map(models, broom::tidy),
    results_glance = map(models, broom::glance)
  ) |>
  select(.id, results_tidy, results_glance) |>
  unnest(results_tidy, results_glance)

boot_straps |>
  group_by(.id, r.squared) |>
  summarize(log_b0_b1 = log(prod(estimate)), .groups = "drop") 

head(boot_straps)
```

Then, the distribution of the estimates for 5000 bootstrap sample is plotted. 
The distribution is approximately normal, being symmetric with respect to 1.04. 

```{r}
boot_straps |>
  filter(term == "tmin") |>
  ggplot(aes(x = estimate)) + 
  geom_density() +
  labs(x = "Estimate", 
       y = "Density", 
       title = "Distribution of the estimates") +
  theme_minimal()
```

Using the 5000 bootstrap estimates, the 2.5% and 97.5% quantiles to provide a 
95% confidence interval for ð‘ŸÌ‚2 and log(ð›½Ì‚0âˆ—ð›½Ì‚1) are also found. 
The confidence interval is (1.005, 1.073). 

```{r}
boot_straps |>
  filter(term == "tmin") |> 
  summarize(
    lower = quantile(estimate, probs = 0.025),
    upper = quantile(estimate, probs = 0.975) 
  ) |>
  knitr::kable(digits = 3)
```



## Problem 2 

The homicides data gathered by The *Washington Post* in 50 large U.S. cities 
is imported and cleaned. An additional variable `city_state` is added to show 
both city and state information in one vector, and a binary variable called 
`solved` is also added to show cases closed with arrest as 1 and the others 
(closed without arrest and open cases) as 0. 

Some cities that do not report victim race, such as Dallas, TX; Phoenix, AZ; 
and Kansas City, MO are omitted, and a data entry with incorrect information 
(e.g. Tulsa, AL) is omitted as well. Data with unknown victim_age or victim_sex 
are also omitted for analysis. The analysis will be limited to the victims 
whose race is white or black. 

```{r homicide_data_import}
homicide_data = read_csv("./data/homicide-data.csv", na = c("NA", "", ".")) |>
  janitor::clean_names() |>
  mutate(city_state = str_c(city, state, sep = ", "), 
         solved = ifelse(disposition == "Closed by arrest", 1, 0)
  ) |>
  filter(!city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL"), 
         victim_race %in% c("White", "Black"), 
         victim_age != "Unknown", 
         victim_sex != "Unknown") |>
  mutate(victim_age = as.numeric(victim_age)) 
```


For the city of Baltimore, MD, the `glm` function is used to fit a logistic 
regression with `solved` as the outcome and `victim_age`, `victim_sex` and 
`victim_race` are predictors. The output of glm function is saved, cleaned 
using the broom::tidy, and the estimate and confidence interval of the adjusted 
odds ratio for solving homicides comparing male victims to female victims, with 
all the other variables fixed, are found.  

```{r baltimore_data}
baltimore_model = 
  homicide_data |>
  filter(city_state == "Baltimore, MD") |>
  glm(solved ~ victim_age + victim_sex + victim_race, data = _) |>
  broom::tidy()

baltimore_model |>
  filter(term =="victim_sexMale") |>
  mutate(
    odds_ratio = exp(estimate), 
    conf_int_lower = exp(estimate - 1.96 * std.error), 
    conf_int_upper = exp(estimate + 1.96 * std.error)
  ) |>
  select(term, estimate, odds_ratio, conf_int_lower, conf_int_upper) |>
  knitr::kable(digit = 3)
```


Then, for each of the 50 cities, the adjusted odds ratio and confidence interval 
for solving homicides comparing male victims to female victims are calculated. 

```{r each_city_data}
city_model = function(name) { 
  
  city_data = 
    homicide_data |>
    filter(city_state == name) 
    
  glm(solved ~ victim_age + victim_sex + victim_race, data = city_data) |>
    broom::tidy()
}

unique_city_state = 
  homicide_data |>
  distinct(city_state) |>
  pull(city_state)

city_simulation = 
  tibble(
    unique_name = unique_city_state
  ) |>
  mutate(data = map(unique_name, city_model)) |>
  unnest(data) 

city_simulation |>
  filter(term == "victim_sexMale") |>
  mutate(
    odds_ratio = exp(estimate), 
    conf_int_lower = exp(estimate - 1.96 * std.error), 
    conf_int_upper = exp(estimate + 1.96 * std.error)
  ) |>
  select(unique_name, estimate, odds_ratio, conf_int_lower, conf_int_upper) |>
  knitr::kable(digit = 3)
```



## Problem 3 

The birthweight data is loaded and cleaned. Missing values are dropped and 
some of the numeric variables are converted to factor variables. For example, 
the race of father and mother is changed from numeric values to appropriate 
categories. 

```{r birthweight_data_import}
birthweight_df = 
  read_csv("./data/birthweight.csv", na = c("NA", "", ".")) |>
  janitor::clean_names() |>
  mutate(
    id = row_number(),
    babysex = case_match(babysex, 
                         1 ~ "male", 
                         2 ~ "female"), 
    babysex = fct_infreq(babysex), 
    frace = case_match(frace, 
                       1 ~ "White", 
                       2 ~ "Black", 
                       3 ~ "Asian", 
                       4 ~ "Puerto Rican", 
                       8 ~ "Other", 
                       9 ~ "Unknown"), 
    frace = fct_infreq(frace),
    malform = case_match(malform, 
                         0 ~ "absent", 
                         1 ~ "present"), 
    malform = fct_infreq(malform),
    mrace = case_match(mrace, 
                       1 ~ "White", 
                       2 ~ "Black", 
                       3 ~ "Asian", 
                       4 ~ "Puerto Rican", 
                       8 ~ "Other", 
                       9 ~ "Unknown"), 
     mrace = fct_infreq(mrace), 
  )
```

To build a regression model for birthweight of this dataset, I will first look 
at some of the factors that seem directly related to the weight of mother and 
some factors that are baby specific. The predictors are as follows: 

* bhead: babyâ€™s head circumference at birth (centimeters)
* blength: babyâ€™s length at birth (centimeteres)
* gaweeks: gestational age in weeks
* delwt: motherâ€™s weight at delivery (pounds)
* mheight: motherâ€™s height (inches)
* ppbmi: motherâ€™s pre-pregnancy BMI
* ppwt: motherâ€™s pre-pregnancy weight (pounds)
* wtgain: motherâ€™s weight gain during pregnancy (pounds)


```{r model}
bwt_model_fit = 
  lm(bwt ~  bhead + blength + gaweeks + delwt + mheight + ppbmi + ppwt + wtgain, 
     data = birthweight_df) 

summary(bwt_model_fit)

par(mfrow = c(2, 2))
plot(bwt_model_fit)

alias(bwt_model_fit)
```

According to the model summary, the adjusted R-squared value is only 0.70. 
And according to the QQ plot, there are few outliers that hinder almost linear 
relationship. Hence, alias function was used to check multilinearity. 
It was found that `delwt`, `ppwt` and `wtgain` are correlated. 

To improve this model, `delwt`, `ppwt` will be removed and only `wtgain` will 
be used as a predictor. 

```{r}
bwt_model_new = 
  lm(bwt ~ bhead + blength + gaweeks + mheight + ppbmi + wtgain, 
     data = birthweight_df) 

summary(bwt_model_new)

alias(bwt_model_new)
car::vif(bwt_model_new)

par(mfrow = c(2, 2))
plot(bwt_model_new)
```

Adjusting the predictors did not improve the adjusted R-squared value but 
successfully removed the alias. VIF was also checked and without any outstanding 
number, there seems to be no multicollinearity issue. However, residual vs leverage
plot is worth noting as it indicates an influential point on the right upper corner. 


To further refine the model, a possible interaction was checked. 

```{r}
bwt_model_int = 
  lm(bwt ~ bhead * blength * gaweeks * mheight * ppbmi * wtgain, 
     data = birthweight_df) 

summary(bwt_model_int)

alias(bwt_model_int)

par(mfrow = c(2, 2))
plot(bwt_model_int)
```

According to this model, the adjusted R sqaured value was slightly higher 
than the previous model. Some significant interaction was checked, but no
multilinearity was found. 

I will use this model to calculate estimated predictions and residuals. 

Once the predictions and residuals are found using `add_predictions` and `add_residuals` functions, these values are plotted against each other. 

```{r plotting_model}
birthweight_df |>
  modelr::add_predictions(bwt_model_int) |>
  modelr::add_residuals(bwt_model_int) |>
  ggplot(aes(x = pred, y = resid)) +
  geom_point()
```


The model built above is compared to two other models using cross validation. 
The two other models are as follows: 

1) One using length at birth and gestational age as predictors (main effects only)

2) One using head circumference, length, sex, and all interactions (including the three-way interaction) between these.


```{r comparison}
cv_df = 
  crossv_mc(birthweight_df, 100) 

cv_df =
  cv_df |> 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))

cv_res_df = 
  cv_df |>
  mutate(
    bwt_my_model = map(train, \(df) lm(bwt ~ bhead * blength * gaweeks * mheight * 
                                         ppbmi * wtgain, data = df)),
    bwt_first_model = map(train, \(df) lm(bwt ~ blength + gaweeks, data = df)), 
    bwt_second_model = map(train, \(df) lm(bwt ~ bhead * blength * babysex, data = df))
  ) |>
  mutate(
    rmse_bwt_my = map2_dbl(bwt_my_model, test, rmse), 
    rmse_bwt_first = map2_dbl(bwt_first_model, test, rmse), 
    rmse_bwt_second = map2_dbl(bwt_second_model, test, rmse)
  )

cv_res_df |>
  dplyr::select(starts_with("rmse")) |>
  pivot_longer(
    everything(), 
    names_to = "model", 
    values_to = "rmse", 
    names_prefix = "rmse_"
  ) |>
  ggplot(aes(x = model, y = rmse)) +
  geom_violin()
```

